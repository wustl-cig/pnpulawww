<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="A Restoration Network as an Implicit Prior">
    <meta name="author" content="Yuyang Hu, Mauricio Delbracio, Peyman Milanfar, Ulugbek S. Kamilov">

    <title>Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior Models</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="src/css/style.css">

</head>

<body>

<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Plug-and-Play Posterior Sampling under</h2><h2>Mismatched Measurement and Prior Models</h2> 
    <p class="abstract"><b>FIRST theoretical analysis of PnP-ULA under mismatched data-fidelity and prior terms</b></p>
    <hr>
    <p class="authors">
        <a href="https://scholar.google.fr/citations?user=NCpjnqwAAAAJ&hl=fr">Marien Renaud<sup>1</sup></a>,
	    <a href="https://jiamingliu-jeremy.github.io">Jiaming Liu<sup>1</sup></a>,
        <a href="https://vdeborto.github.io/">Valentin de Bortoli<sup>2</sup></a>,
        <a href="https://helios2.mi.parisdescartes.fr/~aalmansa/HomePage/">Andres Almansa<sup>3</sup></a>,
        <a href="https://engineering.wustl.edu/faculty/Ulugbek-Kamilov.html">Ulugbek S. Kamilov<sup>1</sup></a></br>
    </p>
    <p>
        <a><sup>1</sup>Washington University in St. Louis, USA</a></br>
        <a><sup>2</sup>ENS Paris, France</a></br>
        <a><sup>3</sup>MAP5, CNRS, France</a></br>
    </p>

    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2310.03546">Preprint</a>
	<a class="btn btn-primary" href="https://openreview.net/forum?id=66arKkGiFy">OpenReview</a>
	<!--<a class="btn btn-primary" href="https://github.com/wustl-cig/DRP">Code</a>-->
    </div>
</div>

<div class="container">
    <div class="section">
    <h2>Abstract </h2>
	      <hr>
        <p>
            Posterior sampling has been shown to be a powerful Bayesian approach for solving imaging inverse problems. The recent plug-and-play unadjusted Langevin algorithm (PnP-ULA) has emerged as a promising method for Monte Carlo sampling and minimum mean squared error (MMSE) estimation by combining physical measurement models with deep-learning priors specified using image denoisers. However, the intricate relationship between the sampling distribution of PnP-ULA and the mismatched data-fidelity and denoiser has not been theoretically analyzed. We address this gap by proposing a posterior-L2 pseudometric and using it to quantify an explicit error bound for PnP-ULA under mismatched posterior distribution. We numerically validate our theory on several inverse problems such as sampling from Gaussian mixture models and image deblurring. Our results suggest that the sensitivity of the sampling distribution of PnP-ULA to a mismatch in the measurement model and the denoiser can be precisely characterized.
    </div>
    <div class="section">
    <h2>Numerical Experiments</h2>
        <div class="row align-items-center">
			<div class="col justify-content-center text-center">
                <img src="img/1-distribution-denoisers.jpg" style="width:90%" alt="Banner">
                <p style="text-align: justify;"><b>Figure 1:</b> Illustration of Theorem 1's bound by visualizing the strong correlation between the Wasserstein distance between sampling distributions and the posterior-L2 distance between denois- ers. <b>Left plot:</b> Distances, for the GMM experiment in 2D, computed between sampling generated by mismatch denoisers and the exact MMSE denoiser. Note how the posterior-L2 is more correlated to the Wasserstein distance than the prior-L2. <b>Right plot:</b> Distances, for the gray-scale images experi- ment, compute between DnCNN denoisers with 5 × 105 weights and other DnCNN denoisers with fewer weights. Note how the posterior-L2 and the Wasserstein distance are highly correlated with correlation r = 0.9909 in average and r > 0.97 for each image.</p>
            </div>
        </div>

        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/2-gmm-experiment.jpg" style="width:90%" alt="Banner">
                <p style="text-align: justify;"><b>Figure 2:</b> Illustration of denoisers with ε = 0.05 used for the 2D Gaussian Mixture experiment. The prior distribution, a Gaussian Mixture, is represented in light blue. Denoisers, Dε : R2 → R2, are represented by there outputs (in dark blue) on a set of inputs (in orange) linked together (by orange lines). <b>Rightmost:</b> Exact MMSE denoiser. <b>Leftmost:</b> Three mismatched denoisers with various c parameters.</p>
            </div>
        </div>

        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/3-denoiser complexity.jpg" style="width:90%" alt="Banner">
                <p style="text-align: justify;"><b>Figure 3:</b> Illustration of MMSE estimators computed by PnP-ULA run on 105 steps with various DnCNN denoisers. The quantities in the top-left corner of each image provide PSNR and SSIM values for each reconstructed images. Denoisers have a different number of weights, but are all trained in the same way. Note that a shift between the reference denoiser (5 × 105 weights) and mismatched denoisers using less weights (103 or 105 weights) implies a shift in the MMSE estimator quality.</p>
            </div>
        </div>

        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/4-forward-model.jpg" style="width:90%" alt="Banner">
                <p style="text-align: justify;"><b>Figure 4:</b> Illustration of the PnP-ULA stability to a mismatch forward model. Leftmost six plots: MMSE estimators computed with PnP-ULA run on 30,000 steps on Gaussian blur. Rightmost: Evolution of the Wasserstein distance between sampling distributions computed with a mismatched blur kernels and sampling with the exact forward model. Note that the image reconstruction quality improves as blur approaches the one used to degrade the image. In addition, in the case of Gaussian blur, the pseudometric justifies qualitatively the observed linear decrease of the Wasserstein distance.</p>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href="https://arxiv.org/abs/2310.03546"
                   class="list-group-item">
                    <img src="img/paper_view.jpg" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @article{renaud2024pnpula,
                author={Marien Renaud 
                  and Jiaming Liu 
                  and Valentin de Bortoli 
                  and Andres Almansa
                  and Ulugbek S. Kamilov},
                title={Plug-and-Play Posterior Sampling under
                    Mismatched Measurement and Prior Models},
                note={Proc. ICLR},
                year={2024}
              }
        </div>
    </div>
    
    <hr>

</div>
</body>
</html>
